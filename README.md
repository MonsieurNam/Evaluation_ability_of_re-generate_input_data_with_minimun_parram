
# ÄÃ¡nh giÃ¡ Kháº£ nÄƒng TÃ¡i táº¡o Input vá»›i Tham sá»‘ Tá»‘i thiá»ƒu

Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c so sÃ¡nh kháº£ nÄƒng cá»§a cÃ¡c kiáº¿n trÃºc mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ¡c nhau (Transformer, RNN, N-gram, Lookup Table) trong viá»‡c **tÃ¡i táº¡o chÃ­nh xÃ¡c cÃ¡c chuá»—i Ä‘áº§u vÃ o Ä‘Ã£ há»c**, vá»›i má»¥c tiÃªu tÃ¬m ra cáº¥u hÃ¬nh mÃ´ hÃ¬nh cÃ³ **sá»‘ lÆ°á»£ng tham sá»‘ huáº¥n luyá»‡n Ä‘Æ°á»£c tá»‘i thiá»ƒu** Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tÃ¡i táº¡o cao (lÃ½ tÆ°á»Ÿng lÃ  100%).

## Má»¥c lá»¥c

1.  [Giá»›i thiá»‡u](#1-giá»›i-thiá»‡u)
2.  [Cáº¥u trÃºc Dá»± Ã¡n](#2-cáº¥u-trÃºc-dá»±-Ã¡n)
3.  [CÃ¡ch Sá»­ dá»¥ng](#3-cÃ¡ch-sá»­-dá»¥ng)
    *   [Giai Ä‘oáº¡n 0: Chuáº©n bá»‹ Dá»¯ liá»‡u](#giai-Ä‘oáº¡n-0-chuáº©n-bá»‹-dá»¯-liá»‡u)
    *   [Giai Ä‘oáº¡n 1: ÄÃ¡nh giÃ¡ Baseline Models](#giai-Ä‘oáº¡n-1-Ä‘Ã¡nh-giÃ¡-baseline-models)
    *   [Giai Ä‘oáº¡n 2: Triá»ƒn khai & Tinh chá»‰nh RNN](#giai-Ä‘oáº¡n-2-triá»ƒn-khai--tinh-chá»‰nh-rnn)
    *   [Giai Ä‘oáº¡n 3: Triá»ƒn khai & Tinh chá»‰nh Transformer (Tiáº¿p theo)](#giai-Ä‘oáº¡n-3-triá»ƒn-khai--tinh-chá»‰nh-transformer-tiáº¿p-theo)
    *   [Giai Ä‘oáº¡n 4: PhÃ¢n tÃ­ch & BÃ¡o cÃ¡o (Cuá»‘i cÃ¹ng)](#giai-Ä‘oáº¡n-4-phÃ¢n-tÃ­ch--bÃ¡o-cÃ¡o-cuá»‘i-cÃ¹ng)
4.  [Káº¿t quáº£ SÆ¡ bá»™](#4-káº¿t-quáº£-sÆ¡-bá»™)
    *   [MÃ´ hÃ¬nh Transformer (Single Block GPT-2)](#mÃ´-hÃ¬nh-transformer-single-block-gpt-2)
    *   [MÃ´ hÃ¬nh RNN (LSTM/GRU)](#mÃ´-hÃ¬nh-rnn-lstm-gru)
5.  [PhÃ¢n tÃ­ch SÆ¡ bá»™ Káº¿t quáº£](#5-phÃ¢n-tÃ­ch-sÆ¡-bá»™-káº¿t-quáº£)



## 1. Giá»›i thiá»‡u

Trong lÄ©nh vá»±c MÃ´ hÃ¬nh NgÃ´n ngá»¯, Transformer Ä‘Ã£ chá»©ng tá» sá»©c máº¡nh vÆ°á»£t trá»™i trong cÃ¡c nhiá»‡m vá»¥ phá»©c táº¡p. Tuy nhiÃªn, Ä‘á»‘i vá»›i má»™t nhiá»‡m vá»¥ Ä‘Æ¡n giáº£n nhÆ° **tÃ¡i táº¡o chÃ­nh xÃ¡c dá»¯ liá»‡u Ä‘Ã£ tháº¥y trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n** (hay cÃ²n gá»i lÃ  "ghi nhá»›"), liá»‡u kiáº¿n trÃºc phá»©c táº¡p nhÆ° Transformer cÃ³ thá»±c sá»± hiá»‡u quáº£ vá» máº·t tham sá»‘ hÆ¡n cÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n nhÆ° RNN, N-gram, hay tháº­m chÃ­ lÃ  má»™t báº£ng tra cá»©u?

Dá»± Ã¡n nÃ y khÃ¡m phÃ¡ cÃ¢u há»i Ä‘Ã³ báº±ng cÃ¡ch triá»ƒn khai vÃ  so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh nÃ y trÃªn má»™t táº­p dá»¯ liá»‡u nhá» gá»“m cÃ¡c cáº·p Há»i-ÄÃ¡p.

## 2. Cáº¥u trÃºc Dá»± Ã¡n

```
.
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ single_block_gpt2.py      # Äá»‹nh nghÄ©a kiáº¿n trÃºc Transformer (GPT-2 single block)
â”‚   â”œâ”€â”€ rnn_model.py              # Äá»‹nh nghÄ©a kiáº¿n trÃºc RNN (Simple LSTM/GRU)
â”‚   â”œâ”€â”€ lookup_table_model.py     # Äá»‹nh nghÄ©a Lookup Table Model (baseline)
â”‚   â””â”€â”€ n_gram_model.py           # Äá»‹nh nghÄ©a N-gram Model (baseline)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py           # Chuáº©n bá»‹ dá»¯ liá»‡u, huáº¥n luyá»‡n tokenizer, tÃ­nh max_seq_len
â”‚   â”œâ”€â”€ train_rnn.py              # Script huáº¥n luyá»‡n mÃ´ hÃ¬nh RNN
â”‚   â”œâ”€â”€ train_transformer.py      # Script huáº¥n luyá»‡n mÃ´ hÃ¬nh Transformer (sáº½ lÃ  training.py cÅ©)
â”‚   â”œâ”€â”€ inference_rnn.py          # Script suy luáº­n vÃ  Ä‘Ã¡nh giÃ¡ RNN
â”‚   â”œâ”€â”€ inference_transformer.py  # Script suy luáº­n vÃ  Ä‘Ã¡nh giÃ¡ Transformer (sáº½ lÃ  inference.py cÅ©)
â”‚   â””â”€â”€ evaluate_baselines.py     # Script Ä‘Ã¡nh giÃ¡ Lookup Table vÃ  N-gram
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data.txt                  # Dá»¯ liá»‡u huáº¥n luyá»‡n thÃ´ (QA_PAIRS tá»« prepare_data.py)
â”‚   â”œâ”€â”€ vocab.json                # Tá»« vá»±ng cá»§a CharacterTokenizer
â”‚   â””â”€â”€ max_seq_len.txt           # Äá»™ dÃ i chuá»—i lá»›n nháº¥t trong data.txt (Ä‘Ã£ mÃ£ hÃ³a)
â”œâ”€â”€ result/                       # ThÆ° má»¥c lÆ°u checkpoint cá»§a Transformer
â”‚   â””â”€â”€ single_block_model_state_dict.pth
â”œâ”€â”€ result_rnn/                   # ThÆ° má»¥c lÆ°u checkpoint cá»§a RNN
â”‚   â””â”€â”€ rnn_model_state_dict.pth
â”œâ”€â”€ results/                      # ThÆ° má»¥c lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ (CSV)
â”‚   â””â”€â”€ baseline_results.csv
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tokenizer.py              # Custom CharacterTokenizer
â””â”€â”€ README.md                     # File nÃ y
```

## 3. CÃ¡ch Sá»­ dá»¥ng

Äá»ƒ cháº¡y dá»± Ã¡n, hÃ£y lÃ m theo cÃ¡c giai Ä‘oáº¡n sau:

### Giai Ä‘oáº¡n 0: Chuáº©n bá»‹ Dá»¯ liá»‡u

BÆ°á»›c nÃ y chuáº©n bá»‹ táº­p dá»¯ liá»‡u, huáº¥n luyá»‡n tokenizer vÃ  xÃ¡c Ä‘á»‹nh Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a.

```bash
python scripts/prepare_data.py
```

*   Káº¿t quáº£: ThÆ° má»¥c `data/` sáº½ Ä‘Æ°á»£c táº¡o chá»©a `data.txt`, `vocab.json`, vÃ  `max_seq_len.txt`.

### Giai Ä‘oáº¡n 1: ÄÃ¡nh giÃ¡ Baseline Models

ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh Lookup Table vÃ  N-gram, cung cáº¥p cÃ¡c baseline vá» hiá»‡u quáº£ tham sá»‘ vÃ  kháº£ nÄƒng tÃ¡i táº¡o.

```bash
python scripts/evaluate_baselines.py
```

*   Káº¿t quáº£: Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ sáº½ Ä‘Æ°á»£c in ra console vÃ  lÆ°u vÃ o `results/baseline_results.csv`.

### Giai Ä‘oáº¡n 2: Triá»ƒn khai & Tinh chá»‰nh RNN

Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh RNN. Má»¥c tiÃªu lÃ  tÃ¬m cáº¥u hÃ¬nh RNN nhá» nháº¥t cÃ³ thá»ƒ tÃ¡i táº¡o 100% dá»¯ liá»‡u.

1.  **Huáº¥n luyá»‡n RNN:**
    Má»Ÿ `scripts/train_rnn.py` vÃ  Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ cáº¥u hÃ¬nh mÃ´ hÃ¬nh (EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS) trong pháº§n `# --- Cáº¥u hÃ¬nh mÃ´ hÃ¬nh RNN ---`. Báº¯t Ä‘áº§u vá»›i cÃ¡c giÃ¡ trá»‹ nhá» (vÃ­ dá»¥: `EMBEDDING_DIM = 4, HIDDEN_SIZE = 4, NUM_LAYERS = 1`) vÃ  tÄƒng dáº§n.

    ```bash
    python scripts/train_rnn.py
    ```

2.  **Suy luáº­n & ÄÃ¡nh giÃ¡ RNN:**
    **Quan trá»ng:** Sau khi huáº¥n luyá»‡n, hÃ£y má»Ÿ `scripts/inference_rnn.py` vÃ  **Ä‘áº·t cÃ¡c giÃ¡ trá»‹ `EMBEDDING_DIM`, `HIDDEN_SIZE`, `NUM_LAYERS` sao cho chÃºng khá»›p chÃ­nh xÃ¡c vá»›i cáº¥u hÃ¬nh báº¡n Ä‘Ã£ dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n trong `train_rnn.py`**.

    ```bash
    python scripts/inference_rnn.py
    ```

*   Káº¿t quáº£: Káº¿t quáº£ huáº¥n luyá»‡n vÃ  suy luáº­n sáº½ Ä‘Æ°á»£c in ra console. Báº¡n sáº½ cáº§n ghi láº¡i cÃ¡c sá»‘ liá»‡u (tham sá»‘, Exact Match Rate, thá»i gian) cho cáº¥u hÃ¬nh RNN tá»‘t nháº¥t.

### Giai Ä‘oáº¡n 3: Triá»ƒn khai & Tinh chá»‰nh Transformer (Tiáº¿p theo)

(ÄÃ¢y lÃ  giai Ä‘oáº¡n tiáº¿p theo trong káº¿ hoáº¡ch cá»§a báº¡n. CÃ¡c script tÆ°Æ¡ng á»©ng lÃ  `scripts/train_transformer.py` (Ä‘á»•i tÃªn tá»« `training.py`) vÃ  `scripts/inference_transformer.py` (Ä‘á»•i tÃªn tá»« `inference.py`). Quy trÃ¬nh tÆ°Æ¡ng tá»± nhÆ° RNN: Ä‘iá»u chá»‰nh cáº¥u hÃ¬nh, huáº¥n luyá»‡n, sau Ä‘Ã³ cáº­p nháº­t cáº¥u hÃ¬nh trong file inference vÃ  Ä‘Ã¡nh giÃ¡.)

### Giai Ä‘oáº¡n 4: PhÃ¢n tÃ­ch & BÃ¡o cÃ¡o (Cuá»‘i cÃ¹ng)

Tá»•ng há»£p táº¥t cáº£ cÃ¡c káº¿t quáº£ tá»« Lookup Table, N-gram, RNN vÃ  Transformer, phÃ¢n tÃ­ch vÃ  trÃ¬nh bÃ y má»™t bÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§.

## 4. Káº¿t quáº£ SÆ¡ bá»™

DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ sÆ¡ bá»™ tá»« cÃ¡c láº§n thá»­ nghiá»‡m gáº§n nháº¥t cá»§a báº¡n. CÃ¡c káº¿t quáº£ nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn CPU vÃ  cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y thuá»™c vÃ o pháº§n cá»©ng vÃ  sá»‘ epoch huáº¥n luyá»‡n.

### MÃ´ hÃ¬nh Transformer (Single Block GPT-2)

(Káº¿t quáº£ tá»« `scripts/inference.py` cÅ©)

```
â–¶ï¸ Inference sáº½ cháº¡y trÃªn device: cpu
Loaded tokenizer from 'results' (vocab_size = 31).
ğŸ”¢ ÄÃ£ Ä‘á»c max_seq_len: 57 (sáº½ dÃ¹ng lÃ m n_positions cho model).
Loaded model state_dict from 'results\single_block_model_state_dict.pth'.
Model cÃ³ tá»•ng cá»™ng 728 tham sá»‘, trong Ä‘Ã³ 728 tham sá»‘ trainable.

--- Báº¯t Ä‘áº§u Kiá»ƒm tra Kháº£ nÄƒng TÃ¡i táº¡o (Reproduction) ---
Tham sá»‘ sinh vÄƒn báº£n: temperature=0.01, top_k=1, top_p=0.95

--- Test Case 1/1 ---
  Prompt: "Question: Xin chÃ o"
  Expected full target length: 57 tokens (from 'Question: Xin chÃ o
Answer: FPT University xin chÃ o báº¡n!.')
  Generating up to 57 tokens...

  ğŸ”¹ Generated Text:
  -----------------------------------------------------------
Question: Xin chÃ o
Answer: FPT University xin chÃ o báº¡n!.
  -----------------------------------------------------------
TÃ¡i táº¡o HOÃ€N TOÃ€N chÃ­nh xÃ¡c!

--- Kiá»ƒm tra Kháº£ nÄƒng TÃ¡i táº¡o HoÃ n táº¥t ---
```

*   **Cáº¥u hÃ¬nh:** (KhÃ´ng hiá»ƒn thá»‹ chi tiáº¿t trong output, nhÆ°ng model Ä‘Ã£ Ä‘áº¡t 100% Exact Match Rate vá»›i 728 tham sá»‘.)
*   **Tá»•ng sá»‘ Tham sá»‘:** 728
*   **Exact Match Rate:** 100.00%
*   **Thá»i gian suy luáº­n trung bÃ¬nh:** (KhÃ´ng cÃ³ trong output nÃ y, nhÆ°ng cÃ³ thá»ƒ Ä‘o trong Giai Ä‘oáº¡n 4)

### MÃ´ hÃ¬nh RNN (LSTM/GRU)

(Káº¿t quáº£ tá»« `scripts/inference_rnn.py` Trial 1: `EMBEDDING_DIM = 4, HIDDEN_SIZE = 4, NUM_LAYERS = 1`)

```
Inference RNN sáº½ cháº¡y trÃªn device: cpu
 Loaded tokenizer from 'result_rnn' (vocab_size = 31).
 ÄÃ£ Ä‘á»c max_seq_len: 57 (dÃ¹ng Ä‘á»ƒ padding vÃ  max_length khi sinh).
 Loaded model state_dict from 'result_rnn\rnn_model_state_dict.pth'.

--- Báº¯t Ä‘áº§u Kiá»ƒm tra Kháº£ nÄƒng TÃ¡i táº¡o (RNN Model) ---
Tham sá»‘ sinh vÄƒn báº£n: temperature=0.01, top_k=1, top_p=0.95

--- Test Case 1/1 ---
  Prompt: "Question: Xin chÃ o"
  Expected full target length: 57 tokens (from 'Question: Xin chÃ o
Answer: FPT University xin chÃ o báº¡n!.')
  Generating up to 57 tokens...

 Generated Text:
  -----------------------------------------------------------
Question: Xin chÃ ouestion: Xin chÃ o
Answer: FPT Universi
  -----------------------------------------------------------
TÃ¡i táº¡o KHÃ”NG chÃ­nh xÃ¡c (Token Accuracy: 0.00%).
Target:   'Question: Xin chÃ o
Answer: FPT University xin chÃ o báº¡n!.'
Generated:'Question: Xin chÃ ouestion: Xin chÃ o
Answer: FPT Universi'

--- TÃ³m táº¯t ÄÃ¡nh giÃ¡ RNN Model ---
  Tá»•ng sá»‘ test cases: 1
  Sá»‘ lÆ°á»£ng tÃ¡i táº¡o chÃ­nh xÃ¡c hoÃ n toÃ n: 0
  Exact Match Rate: 0.00%
  Thá»i gian suy luáº­n trung bÃ¬nh: 40.338 ms/chuá»—i
```

*   **Cáº¥u hÃ¬nh:** `EMBEDDING_DIM = 4, HIDDEN_SIZE = 4, NUM_LAYERS = 1`
*   **Tá»•ng sá»‘ Tham sá»‘:** (Báº¡n sáº½ tháº¥y trong output cá»§a `train_rnn.py`, vÃ­ dá»¥ khoáº£ng vÃ i trÄƒm. Cáº§n cháº¡y `train_rnn.py` vá»›i cáº¥u hÃ¬nh nÃ y vÃ  ghi láº¡i tá»« `torchinfo.summary`.)
*   **Exact Match Rate:** 0.00% (cho cáº¥u hÃ¬nh nÃ y)
*   **Thá»i gian suy luáº­n trung bÃ¬nh:** 40.338 ms/chuá»—i

## 5. PhÃ¢n tÃ­ch SÆ¡ bá»™ Káº¿t quáº£

Tá»« cÃ¡c káº¿t quáº£ sÆ¡ bá»™, cÃ³ thá»ƒ tháº¥y:

*   **MÃ´ hÃ¬nh Transformer** vá»›i cáº¥u hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n (Ä‘Æ°á»£c lÆ°u trong `result/`) Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c kháº£ nÄƒng tÃ¡i táº¡o **hoÃ n háº£o (100% Exact Match Rate)** cho test case Ä‘áº§u tiÃªn. Äiá»u nÃ y cho tháº¥y nÃ³ Ä‘Ã£ há»c thuá»™c chuá»—i Ä‘áº§u vÃ o ráº¥t tá»‘t.
*   **MÃ´ hÃ¬nh RNN** vá»›i cáº¥u hÃ¬nh `(EMBEDDING_DIM=4, HIDDEN_SIZE=4, NUM_LAYERS=1)` Ä‘Ã£ **tháº¥t báº¡i hoÃ n toÃ n** trong viá»‡c tÃ¡i táº¡o. Äiá»u nÃ y khÃ´ng gÃ¢y ngáº¡c nhiÃªn vÃ¬ cáº¥u hÃ¬nh nÃ y cá»±c ká»³ nhá» vÃ  cÃ³ thá»ƒ khÃ´ng Ä‘á»§ nÄƒng lá»±c Ä‘á»ƒ ghi nhá»› cÃ¡c chuá»—i dÃ i vÃ  Ä‘a dáº¡ng.

**BÆ°á»›c tiáº¿p theo cho RNN:**  cáº§n tiáº¿p tá»¥c thá»­ nghiá»‡m vá»›i cÃ¡c cáº¥u hÃ¬nh RNN lá»›n hÆ¡n (tÄƒng `EMBEDDING_DIM` vÃ /hoáº·c `HIDDEN_SIZE`, vÃ  cÃ³ thá»ƒ `NUM_LAYERS`) cho Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c 100% Exact Match Rate. Sau Ä‘Ã³, báº¡n sáº½ so sÃ¡nh sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cáº¥u hÃ¬nh RNN nhá» nháº¥t Ä‘Ã³ vá»›i mÃ´ hÃ¬nh Transformer Ä‘á»ƒ xem mÃ´ hÃ¬nh nÃ o hiá»‡u quáº£ hÆ¡n vá» tham sá»‘ cho nhiá»‡m vá»¥ ghi nhá»› nÃ y.

---