# File: inference.py

import os
import torch
import sys

# Th√™m project root v√†o sys.path
current_dir = os.path.dirname(__file__)
project_root = os.path.abspath(os.path.join(current_dir, '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from utils.tokenizer import CharacterTokenizer
from models.single_block_gpt2 import SingleBlockGPT2ModelNoDepend, GPT2Config
from prepare_data import QA_PAIRS  # Import QA_PAIRS t·ª´ prepare_data.py

# Th∆∞ m·ª•c ch·ª©a model v√† tokenizer ƒë√£ train xong
TRAINED_SINGLE_BLOCK_MODEL_PATH = 'results'
MODEL_STATE_DICT_PATH = os.path.join(TRAINED_SINGLE_BLOCK_MODEL_PATH, 'single_block_model_state_dict.pth')
DATA_PATH = './data'  # Th∆∞ m·ª•c ch·ª©a data.txt, vocab.json, max_seq_len.txt
MAX_SEQ_LEN_FILE = os.path.join(DATA_PATH, 'max_seq_len.txt')


def load_trained_single_block_model(model_dir: str, state_dict_path: str, max_seq_len_file: str, device: torch.device):
    """
    - Load tokenizer t·ª´ model_dir.
    - ƒê·ªçc max_seq_len t·ª´ file.
    - T·∫°o config d·ª±a tr√™n vocab_size c·ªßa tokenizer v√† max_seq_len.
    - Kh·ªüi t·∫°o SingleBlockGPT2ModelNoDepend v·ªõi config ƒë√≥.
    - Load state_dict t·ª´ state_dict_path.
    - Tr·∫£ v·ªÅ (model, tokenizer, max_seq_len).
    """
    # 1. Load tokenizer
    try:
        tokenizer = CharacterTokenizer.from_pretrained(model_dir)
        print(f"Loaded tokenizer from '{model_dir}' (vocab_size = {tokenizer.vocab_size}).")
    except Exception as e:
        print(f"Error loading tokenizer from '{model_dir}': {e}")
        return None, None, None

    # 2. ƒê·ªçc max_seq_len t·ª´ file
    if not os.path.exists(max_seq_len_file):
        print(f"Kh√¥ng t√¨m th·∫•y file max_seq_len.txt t·∫°i '{max_seq_len_file}'.")
        return None, None, None
    with open(max_seq_len_file, 'r') as f:
        max_seq_len = int(f.read().strip())
    print(f"üî¢ ƒê√£ ƒë·ªçc max_seq_len: {max_seq_len} (s·∫Ω d√πng l√†m n_positions cho model).")

    # 3. T·∫°o config d√πng vocab_size v√† n_positions v·ª´a load
    config = GPT2Config(vocab_size=tokenizer.vocab_size, n_positions=max_seq_len)

    # 4. Kh·ªüi t·∫°o model
    model = SingleBlockGPT2ModelNoDepend(config)

    # 5. Load state_dict (checkpoint ƒë√£ train)
    if not os.path.exists(state_dict_path):
        print(f"Kh√¥ng t√¨m th·∫•y file state_dict t·∫°i '{state_dict_path}'.")
        return None, None, None

    try:
        ckpt = torch.load(state_dict_path, map_location=device)
        model.load_state_dict(ckpt)
        print(f"Loaded model state_dict from '{state_dict_path}'.")
    except Exception as e:
        print(f"Error loading state_dict: {e}")
        return None, None, None

    # 6. ƒê∆∞a model v·ªÅ ƒë√∫ng device v√† set eval mode
    model.to(device)
    model.eval()

    return model, tokenizer, max_seq_len


def count_model_params(model: torch.nn.Module):
    """
    ƒê·∫øm t·ªïng s·ªë tham s·ªë (parameters) v√† s·ªë tham s·ªë trainable (requires_grad=True).
    Tr·∫£ v·ªÅ (total_params, trainable_params)
    """
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def generate_text(
    model: SingleBlockGPT2ModelNoDepend,
    tokenizer: CharacterTokenizer,
    prompt: str,
    max_length: int,  # T·ªïng s·ªë token ƒë·∫ßu ra (bao g·ªìm prompt)
    temperature: float,
    top_k: int,
    top_p: float,
    device: torch.device
) -> str:
    """
    - M√¥ t·∫£: sinh vƒÉn b·∫£n d·ª±a tr√™n model ƒë√£ train v√† tokenizer.
    - prompt: chu·ªói ƒë·∫ßu v√†o (v√≠ d·ª•: "Question: Xin ch√†o").
    """
    token_id_list = tokenizer.encode(prompt)
    input_ids = torch.tensor([token_id_list], dtype=torch.long, device=device)

    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=input_ids,
            max_length=max_length,
            pad_token_id=tokenizer.pad_token_id,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            device=device
        )

    generated_ids_list = generated_ids[0].tolist()
    generated_text = tokenizer.decode(generated_ids_list)

    return generated_text


if __name__ == '__main__':
    # 1. Thi·∫øt l·∫≠p device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"‚ñ∂Ô∏è Inference s·∫Ω ch·∫°y tr√™n device: {device}")

    # 2. Load model + tokenizer + max_seq_len ƒë√£ train
    model, tokenizer, max_seq_len_from_training = load_trained_single_block_model(
        TRAINED_SINGLE_BLOCK_MODEL_PATH,
        MODEL_STATE_DICT_PATH,
        MAX_SEQ_LEN_FILE,
        device
    )

    if model is None or tokenizer is None:
        print("Failed to load model/tokenizer. K·∫øt th√∫c inference.")
        exit(1)

    # 2.1. In s·ªë l∆∞·ª£ng tham s·ªë c·ªßa model
    total_params, trainable_params = count_model_params(model)
    print(f"Model c√≥ t·ªïng c·ªông {total_params:,} tham s·ªë, trong ƒë√≥ {trainable_params:,} tham s·ªë trainable.\n")

    # 3. ƒê·ªãnh nghƒ©a c√°c c·∫∑p prompt v√† full_target t∆∞∆°ng ·ª©ng
    #    ƒê·ªÉ t√°i t·∫°o input ch√≠nh x√°c, ch√∫ng ta c·∫ßn bi·∫øt full_target c·ªßa t·ª´ng prompt.
    test_cases = []
    for qa_pair in QA_PAIRS:
        parts = qa_pair.split('\nAnswer:', 1)
        if len(parts) == 2:
            prompt_q = parts[0].strip()  # "Question: Xin ch√†o"
            full_target_qa = qa_pair.strip()  # "Question: Xin ch√†o \nAnswer: FPT University xin ch√†o b·∫°n!."
            test_cases.append({
                'prompt': prompt_q,
                'full_target': full_target_qa,
                'target_length': len(tokenizer.encode(full_target_qa))
            })
        else:
            # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p kh√¥ng ph·∫£i Q&A n·∫øu c√≥ (v√≠ d·ª•: "Python is a programming language.")
            # Coi c·∫£ c√¢u l√† prompt v√† target n·∫øu kh√¥ng c√≥ \nAnswer:
            test_cases.append({
                'prompt': qa_pair.strip(),
                'full_target': qa_pair.strip(),
                'target_length': len(tokenizer.encode(qa_pair.strip()))
            })

    # 4. C√°c tham s·ªë sinh vƒÉn b·∫£n ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªÉ ∆∞u ti√™n t√°i t·∫°o ch√≠nh x√°c
    temperature = 0.01  # R·∫•t th·∫•p, g·∫ßn greedy decoding
    top_k = 1  # Lu√¥n ch·ªçn token c√≥ x√°c su·∫•t cao nh·∫•t
    top_p = 0.95  # S·∫Ω kh√¥ng ·∫£nh h∆∞·ªüng nhi·ªÅu khi top_k=1 v√† temperature th·∫•p

    print(f"\n--- B·∫Øt ƒë·∫ßu Ki·ªÉm tra Kh·∫£ nƒÉng T√°i t·∫°o (Reproduction) ---")
    print(f"Tham s·ªë sinh vƒÉn b·∫£n: temperature={temperature}, top_k={top_k}, top_p={top_p}\n")

    for i, test_case in enumerate(test_cases):
        input_prompt = test_case['prompt']
        full_target_text = test_case['full_target']
        max_gen_length = test_case['target_length']  # ƒê·∫£m b·∫£o sinh ƒë·ªß ƒë·ªô d√†i c·ªßa c·∫∑p QA ƒë√£ h·ªçc

        print(f"\n--- Test Case {i+1}/{len(test_cases)} ---")
        print(f"  Prompt: \"{input_prompt}\"")
        print(f"  Expected full target length: {max_gen_length} tokens "
              f"(from '{full_target_text}')")
        print(f"  Generating up to {max_gen_length} tokens...\n")

        generated = generate_text(
            model=model,
            tokenizer=tokenizer,
            prompt=input_prompt,
            max_length=max_gen_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            device=device
        )

        print("  üîπ Generated Text:")
        print("  -----------------------------------------------------------")
        print(generated)
        print("  -----------------------------------------------------------")

        # ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c t√°i t·∫°o
        if generated == full_target_text:
            print("T√°i t·∫°o HO√ÄN TO√ÄN ch√≠nh x√°c!")
        else:
            print("T√°i t·∫°o KH√îNG ch√≠nh x√°c.")
            print(f"  Target:   '{full_target_text}'")
            print(f"  Generated:'{generated}'")
            # B·∫°n c√≥ th·ªÉ th√™m logic t√≠nh token accuracy ·ªü ƒë√¢y n·∫øu mu·ªën chi ti·∫øt h∆°n

    print("\n--- Ki·ªÉm tra Kh·∫£ nƒÉng T√°i t·∫°o Ho√†n t·∫•t ---")

    # 5. D·ªçn d·∫πp
    del model
    del tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
