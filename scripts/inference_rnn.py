# scripts/inference_rnn.py

import os
import torch
import sys
import time

current_dir = os.path.dirname(__file__)
project_root = os.path.abspath(os.path.join(current_dir, '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

from utils.tokenizer import CharacterTokenizer
from models.rnn_model import SimpleRNNModel
from prepare_data import QA_PAIRS # ƒê·ªÉ l·∫•y c√°c c·∫∑p Q&A g·ªëc

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
DATA_PATH = './data'
MAX_SEQ_LEN_FILE = os.path.join(DATA_PATH, 'max_seq_len.txt')
TRAINED_RNN_MODEL_PATH = 'result_rnn' # Th∆∞ m·ª•c ch·ª©a model v√† tokenizer ƒë√£ train xong
MODEL_STATE_DICT_PATH = os.path.join(TRAINED_RNN_MODEL_PATH, 'rnn_model_state_dict.pth')


def load_trained_rnn_model(model_dir: str, state_dict_path: str, max_seq_len_file: str, device: torch.device):
    """
    - Load tokenizer t·ª´ model_dir.
    - ƒê·ªçc max_seq_len t·ª´ file.
    - Kh·ªüi t·∫°o SimpleRNNModel v·ªõi c·∫•u h√¨nh ƒë√£ d√πng khi train.
    - Load state_dict t·ª´ state_dict_path.
    - Tr·∫£ v·ªÅ (model, tokenizer, max_seq_len).
    """
    # 1. Load tokenizer
    try:
        tokenizer = CharacterTokenizer.from_pretrained(model_dir)
        print(f" Loaded tokenizer from '{model_dir}' (vocab_size = {tokenizer.vocab_size}).")
    except Exception as e:
        print(f" Error loading tokenizer from '{model_dir}': {e}")
        return None, None, None

    # 2. ƒê·ªçc max_seq_len
    if not os.path.exists(max_seq_len_file):
        print(f" Kh√¥ng t√¨m th·∫•y file max_seq_len.txt t·∫°i '{max_seq_len_file}'.")
        return None, None, None
    with open(max_seq_len_file, 'r') as f:
        max_seq_len = int(f.read().strip())
    print(f" ƒê√£ ƒë·ªçc max_seq_len: {max_seq_len} (d√πng ƒë·ªÉ padding v√† max_length khi sinh).")

    # 3. Kh·ªüi t·∫°o model v·ªõi c·∫•u h√¨nh ph√π h·ª£p (PH·∫¢I TR√ôNG V·ªöI C·∫§U H√åNH KHI TRAIN)
    #    ƒê·ªÉ th·ª±c t·∫ø, b·∫°n c√≥ th·ªÉ l∆∞u c·∫•u h√¨nh v√†o m·ªôt file JSON khi train,
    #    sau ƒë√≥ ƒë·ªçc l·∫°i ·ªü ƒë√¢y. Hi·ªán t·∫°i, ta s·∫Ω gi·∫£ ƒë·ªãnh c·∫•u h√¨nh n√†y
    #    tr√πng v·ªõi c·∫•u h√¨nh trong train_rnn.py.
    #    N·∫øu mu·ªën thay ƒë·ªïi, h√£y thay ƒë·ªïi trong c·∫£ 2 file.
    EMBEDDING_DIM = 4
    HIDDEN_SIZE = 16
    NUM_LAYERS = 1

    model = SimpleRNNModel(
        vocab_size=tokenizer.vocab_size,
        embedding_dim=EMBEDDING_DIM,
        hidden_size=HIDDEN_SIZE,
        num_layers=NUM_LAYERS,
        pad_token_id=tokenizer.pad_token_id
    )

    # 4. Load state_dict
    if not os.path.exists(state_dict_path):
        print(f" Kh√¥ng t√¨m th·∫•y file state_dict t·∫°i '{state_dict_path}'.")
        return None, None, None

    try:
        ckpt = torch.load(state_dict_path, map_location=device)
        model.load_state_dict(ckpt)
        print(f" Loaded model state_dict from '{state_dict_path}'.")
    except Exception as e:
        print(f" Error loading state_dict: {e}")
        return None, None, None

    # 5. ƒê∆∞a model v·ªÅ ƒë√∫ng device v√† set eval mode
    model.to(device)
    model.eval()

    return model, tokenizer, max_seq_len


def calculate_accuracy(generated_text: str, target_text: str, tokenizer: CharacterTokenizer):
    """T√≠nh token accuracy v√† exact match."""
    generated_ids = tokenizer.encode(generated_text)
    target_ids = tokenizer.encode(target_text)

    # ƒê·∫£m b·∫£o ƒë·ªô d√†i b·∫±ng nhau ƒë·ªÉ so s√°nh
    min_len = min(len(generated_ids), len(target_ids))
    correct_tokens = 0
    for i in range(min_len):
        if generated_ids[i] == target_ids[i]:
            correct_tokens += 1
    
    token_accuracy = (correct_tokens / len(target_ids)) * 100 if len(target_ids) > 0 else 0
    exact_match = (generated_text == target_text)

    return token_accuracy, exact_match


if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Inference RNN s·∫Ω ch·∫°y tr√™n device: {device}")

    # Load model + tokenizer + max_seq_len ƒë√£ train
    model, tokenizer, max_seq_len_from_training = load_trained_rnn_model(
        TRAINED_RNN_MODEL_PATH,
        MODEL_STATE_DICT_PATH,
        MAX_SEQ_LEN_FILE,
        device
    )

    if model is None or tokenizer is None:
        print("Failed to load model/tokenizer. K·∫øt th√∫c inference.")
        exit(1)

    # Chu·∫©n b·ªã test cases t·ª´ QA_PAIRS (t∆∞∆°ng t·ª± prepare_data v√† evaluate_baselines)
    test_cases = []
    for qa_pair in QA_PAIRS:
        parts = qa_pair.split('\nAnswer:', 1)
        if len(parts) == 2:
            prompt_q = parts[0].strip()
            full_target_qa = qa_pair.strip()
            test_cases.append({
                'prompt': prompt_q,
                'full_target': full_target_qa,
                'target_length': len(tokenizer.encode(full_target_qa))
            })
        else:
            test_cases.append({
                'prompt': qa_pair.strip(),
                'full_target': qa_pair.strip(),
                'target_length': len(tokenizer.encode(qa_pair.strip()))
            })

    # C√°c tham s·ªë sinh vƒÉn b·∫£n ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªÉ ∆∞u ti√™n t√°i t·∫°o ch√≠nh x√°c
    temperature = 0.01
    top_k = 1
    top_p = 0.95

    print(f"\n--- B·∫Øt ƒë·∫ßu Ki·ªÉm tra Kh·∫£ nƒÉng T√°i t·∫°o (RNN Model) ---")
    print(f"Tham s·ªë sinh vƒÉn b·∫£n: temperature={temperature}, top_k={top_k}, top_p={top_p}\n")
    
    total_exact_matches = 0
    total_inference_time_ms = 0

    for i, test_case in enumerate(test_cases):
        input_prompt = test_case['prompt']
        full_target_text = test_case['full_target']
        # ƒê·∫£m b·∫£o max_gen_length kh√¥ng v∆∞·ª£t qu√° max_seq_len m√† model ƒë∆∞·ª£c train
        # Tuy nhi√™n, n·∫øu model ƒë∆∞·ª£c train tr√™n t·ª´ng c·∫∑p QA, th√¨ max_gen_length ch√≠nh l√† target_length
        # c·ªßa c·∫∑p QA ƒë√≥.
        max_gen_length = test_case['target_length'] 

        print(f"\n--- Test Case {i+1}/{len(test_cases)} ---")
        print(f"  Prompt: \"{input_prompt}\"")
        print(f"  Expected full target length: {max_gen_length} tokens (from '{full_target_text}')")
        print(f"  Generating up to {max_gen_length} tokens...\n")

        start_time = time.time()
        generated = model.generate(
            input_ids=torch.tensor([tokenizer.encode(input_prompt)], dtype=torch.long),
            max_length=max_gen_length,
            pad_token_id=tokenizer.pad_token_id,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            device=device
        )
        end_time = time.time()
        total_inference_time_ms += (end_time - start_time) * 1000

        print("  üîπ Generated Text:")
        print("  -----------------------------------------------------------")
        print(generated)
        print("  -----------------------------------------------------------")

        # ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c t√°i t·∫°o
        token_acc, exact_match = calculate_accuracy(generated, full_target_text, tokenizer)
        if exact_match:
            total_exact_matches += 1
            print("T√°i t·∫°o HO√ÄN TO√ÄN ch√≠nh x√°c!")
        else:
            print(f"T√°i t·∫°o KH√îNG ch√≠nh x√°c (Token Accuracy: {token_acc:.2f}%).")
            print(f"Target:   '{full_target_text}'")
            print(f"Generated:'{generated}'")
    
    avg_inference_time_ms = total_inference_time_ms / len(test_cases)
    exact_match_rate = (total_exact_matches / len(test_cases)) * 100

    print(f"\n--- T√≥m t·∫Øt ƒê√°nh gi√° RNN Model ---")
    print(f"  T·ªïng s·ªë test cases: {len(test_cases)}")
    print(f"  S·ªë l∆∞·ª£ng t√°i t·∫°o ch√≠nh x√°c ho√†n to√†n: {total_exact_matches}")
    print(f"  Exact Match Rate: {exact_match_rate:.2f}%")
    print(f"  Th·ªùi gian suy lu·∫≠n trung b√¨nh: {avg_inference_time_ms:.3f} ms/chu·ªói")

    # TODO: L∆∞u k·∫øt qu·∫£ v√†o file CSV (khi ƒë√£ ho√†n th√†nh c·∫£ Transformer)
    # ƒê·ªÉ tr√°nh ghi ƒë√®, ta s·∫Ω l∆∞u k·∫øt qu·∫£ c·ªßa RNN v√† Transformer v√†o m·ªôt file chung sau Giai ƒëo·∫°n 4.

    # D·ªçn d·∫πp
    del model
    del tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()